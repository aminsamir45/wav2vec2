# Getting Started

## ğŸš€ Quick Setup

### 1. Install Dependencies

```bash
# Option 1: Using pip
pip install -r requirements.txt

# Option 2: Using setup.py
pip install -e .
```

### 2. Download Original Models and Data

```bash
# Download pre-trained models and LibriSpeech data
python scripts/setup_original_wav2vec2.py
```

### 3. Run Baseline Experiments

```bash
# Test the baseline implementation
python src/baseline/wav2vec2_baseline.py
```

## ğŸ“ Project Structure

```
wav2vec2/
â”œâ”€â”€ PROJECT_PLAN.md           # Project overview and goals
â”œâ”€â”€ GETTING_STARTED.md        # This file
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ setup.py                  # Package setup
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ baseline/
â”‚       â””â”€â”€ wav2vec2_baseline.py   # Baseline implementation
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ setup_original_wav2vec2.py # Setup script
â””â”€â”€ models/
    â””â”€â”€ pretrained/           # Downloaded models (created by setup)
```

## ğŸ”§ Original wav2vec 2.0 Code Access

The original wav2vec 2.0 code is available at:
- **Paper**: https://arxiv.org/pdf/2006.11477
- **Code**: https://github.com/pytorch/fairseq (fairseq implementation)
- **Models**: https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec

Our baseline uses the HuggingFace transformers implementation, which is more accessible and easier to modify.

## ğŸ¯ Next Steps

1. **Run baseline**: Verify everything works with the baseline
2. **Check results**: Compare your baseline results with paper results
3. **Start experimenting**: Begin implementing improvements in `src/improvements/`

## ğŸ’» Commands to Run

```bash
# Install everything
pip install -r requirements.txt

# Download models and data  
python scripts/setup_original_wav2vec2.py

# Run baseline test
python src/baseline/wav2vec2_baseline.py
```

That's it! You're ready to start experimenting with wav2vec 2.0 improvements. 